### 背景

关系图谱这个功能的背景，是通过查询某一设备以及该设备连接或者扫描的wifi或者wg进行选中，从而扩出同网设备，以达到团伙窝点进行抓捕。如果不进行一些相关优化或者限制，该功能的查询出来的量是非常大的，浏览器渲染以及buff层服务都是会崩的，这是该功能的技术优化背景

这种关系图谱的查询优化确实是个关键问题，尤其是针对公安大数据场景，查询数据量极大，关系扩展容易导致爆炸式增长，最终导致浏览器渲染和服务端崩溃。因此，优化思路可以从以下几个方面入手：

### **1. 查询优化**

#### **(1) 限制查询深度和宽度**

- **深度限制**：限制图谱查询的最大层级，避免无限扩展。

- **宽度限制**：对单个节点扩展出的子节点数量进行限制，例如最多扩展 500 个设备，超出的部分按权重筛选（如连接时间长短、信号强度等）。

- **分页加载**：不一次性加载所有数据，而是按需加载，默认展示一层数据，用户可手动展开。

#### **(2) 数据裁剪与去重**

- **时间范围**：查询条件中加入时间限制，例如查询最近 30 天的数据，而非全量数据。

- **连接权重**：根据设备与 WiFi/WG 之间的关联度（如信号强度、连接时间）进行筛选，去掉权重低的边。

- **去重处理**：对设备 ID、WiFi/WG ID 进行去重，避免重复计算导致数据量膨胀。

#### **(3) 预计算和索引优化**

- **预计算热点数据**：对高频查询的热点区域、热点设备提前计算存储，减少实时查询压力。

- **索引优化**：针对设备 ID、WiFi/WG ID、时间戳等字段建立合适的索引，提高查询效率。

---

### **2. 服务端优化**

#### **(1) 分批返回数据**

- **流式数据返回**：通过 WebSocket 或 SSE（Server-Sent Events）让前端按批次接收数据，而不是一次性返回大数据量。

- **后端分页**：查询结果按分页方式返回，每次仅返回 500~1000 条数据，前端滚动加载。

#### **(2) 缓存机制**

- **短期缓存**：对相同查询条件的结果进行缓存，避免重复查询数据库。

- **热点数据缓存**：将高频查询的设备/网络数据缓存到 Redis，提高查询响应速度。

#### **(3) 异步计算**

- **任务队列**：对于大规模查询，可以用消息队列（Kafka、RabbitMQ）异步处理，避免阻塞主业务。

- **Graph DB 支持**：使用专门的图数据库（如 Neo4j、JanusGraph）来优化图查询性能，避免传统关系数据库在复杂关系查询上的瓶颈。
  
  - 现在是使用 hbase + clickhouse 进行查询的

---

### **3. 前端优化**

#### **(1) 渲染性能优化**

- **虚拟列表**：仅渲染视口范围内的节点，避免一次性渲染上万条数据。

- **Canvas/WebGL 渲染**：使用 PixiJS 或 Cytoscape.js 进行高性能图谱渲染，避免 DOM 过载。

- **LOD（Level of Detail）优化**：根据缩放级别动态调整显示的节点数量，缩小时只显示关键节点，放大后才加载详细数据。

#### **(2) 用户交互优化**

- **懒加载**：仅显示核心节点，允许用户手动展开更多关联节点。

- **高亮重点关系**：对查询目标的直接关联设备优先展示，非重点设备可以折叠或低透明度显示。

- **筛选 & 搜索**：提供筛选条件（如设备类别、连接时间等）减少无效数据渲染。

---

### **4. 总结**

- **数据层优化**：深度、宽度限制 + 预计算 + 索引优化

- **服务层优化**：流式返回 + 分批查询 + 缓存策略

- **前端优化**：虚拟渲染 + WebGL 支持 + LOD 级别优化

这些优化手段可以有效降低查询压力，提高系统稳定性，同时保证查询结果的可视化渲染流畅度。你们目前在哪个环节遇到瓶颈？
